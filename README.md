# ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation

## ğŸ“‚ é¡¹ç›®ç»“æ„
```
â”œâ”€â”€ main.py                 # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ config/                 # Hydra é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ config.yaml         # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ src/                    
â”‚   â”œâ”€â”€ argfocus.py         # ArgusCogito_focus (bboxç”Ÿæˆ)
â”‚   â”œâ”€â”€ knowledge.py        # Knowledge factory (çŸ¥è¯†æå–/ç”Ÿæˆ)
â”‚   â”œâ”€â”€ segment.py          # Sam4Segment (åˆ†å‰²æ¨¡å—)
â””â”€â”€ requirements.txt        # ä¾èµ–åº“
```

---

## âš™ï¸ ç¯å¢ƒä¾èµ–
- Python >= 3.9  
- PyTorch >= 2.0 (æ”¯æŒ GPU)  
- ä¾èµ–åº“ï¼š
  ```bash
  pip install -r requirements.txt
  ```
  ä¸»è¦ä¾èµ–ï¼š
  - transformers  
  - peft  
  - Pillow  
  - hydra-core  

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å‡†å¤‡è¾“å…¥æ•°æ®
- å°† **RGB å›¾åƒ** å’Œ **Depth å›¾åƒ** æ”¾å…¥ `config.yaml` æŒ‡å®šçš„è·¯å¾„  
- æ”¯æŒå•å¼ å›¾åƒæˆ–æ‰¹é‡å¤„ç†  

### 2. ä¿®æ”¹é…ç½®æ–‡ä»¶
åœ¨ `config/config.yaml` ä¸­è®¾ç½®ï¼š
```yaml
image: null  # image to process only

# If you specify an image, only this image will be inferred (only basename, no extention)

use_knowledge: True
use_knowledge_origin: False # use the knowledge already have
use_bboxes_origin: False

use_sam4mllm: True # use sam4mllm to provide point hints
save_knowlwdge: True
save_bboxes: True

focus_type: left_middle_right

target_long_edge: 1000
max_new_tokens: 1024

input:
  knowledge_origin: # The folder for knowledge of the images
  RGB_image: # dataset rgb image
  Depth_image: # dataset depth image

output:
  log_file: # .log file for logging
  save_file: # folder for saving visible images and masks
  save_knowledge: # folder for saving knowledge
  save_bboxes: # folder for saving bboxes

model:
  qwen2_vl_path: # model path
  sam2_ckpt: facebook/sam2.1-hiera-large # dafault path for sam2
  llava_ckpt: # model path
  sam4mllm_ckpt: /disk4/tan/SAM4MLLM-main/checkpoint/sam4mllm


rounds:
  - id: 1
    words: ['animal or human']
  - id: 2
    words: ['animal or human']


rgb_image_extention: .jpg
depth_image_extention: .png
```

### 3. è¿è¡Œ
```bash
python main.py
```

æ”¯æŒå•å¼ å›¾åƒå¤„ç†ï¼š
```bash
python main.py image=example_001
```

## æ¨¡å‹ä½¿ç”¨ & ä¸‹è½½é“¾æ¥

- `llava-next`ï¼š[llava-next](https://huggingface.co/lmms-lab/llama3-llava-next-8b)
- `sam4mllm+`ï¼š[sam4mllm+](https://drive.google.com/drive/folders/1ytEfGRa6bxThTXQn5MLVKKy4jsxxBo6M)
- `Qwen2.5-VL-7B-Instruct`: [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)
- `sam2.1-hiera-large`: ç›´æ¥ä½¿ç”¨ facebook/sam2.1-hiera-large è¿›è¡Œæ‹‰å–

## å¼•ç”¨

æœ¬é¡¹ç›®ä½¿ç”¨äº†ä»¥ä¸‹ç ”ç©¶æˆæœï¼Œè‹¥æ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨æœ¬é¡¹ç›®ï¼Œè¯·åŒæ—¶å¼•ç”¨åŸå§‹è®ºæ–‡ï¼š

```bibtex
@inproceedings{chen2024sam4mllm,
  title={Sam4mllm: Enhance multi-modal large language model for referring expression segmentation},
  author={Chen, Yi-Chia and Li, Wei-Hua and Sun, Cheng and Wang, Yu-Chiang Frank and Chen, Chu-Song},
  booktitle={European Conference on Computer Vision},
  pages={323--340},
  year={2024},
  organization={Springer}
}

